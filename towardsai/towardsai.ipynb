{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd0e3cc630a27171bda4963781135d38e3c4b2db62d9414523c30df1024dbff97fb",
   "display_name": "Python 3.8.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"Natural_Language_Processing_Text.txt\")\n",
    "text = text_file.read()\n",
    "print(text)"
   ]
  },
  {
   "source": [
    "# Basic crunch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenizing\n",
    "sentences = sent_tokenize(text)\n",
    "print(len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenizing\n",
    "words = word_tokenize(text)\n",
    "print(len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency distribution\n",
    "fdist = FreqDist(words)\n",
    "print(fdist.most_common(10))\n",
    "fdist.plot(10)"
   ]
  },
  {
   "source": [
    "# Removing punctuations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation marks\n",
    "words_no_punc = [w.lower() for w in words if w.isalpha()]\n",
    "print(len(words_no_punc))\n",
    "print(words_no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting without punctuation\n",
    "fdist_no_punc = FreqDist(words_no_punc)\n",
    "print(fdist_no_punc.most_common(10))\n",
    "fdist_no_punc.plot(10)"
   ]
  },
  {
   "source": [
    "# Stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words(\"english\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "clean_words = [w for w in words_no_punc if w not in stopwords]\n",
    "print(len(clean_words))\n",
    "print(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_clean = FreqDist(clean_words)\n",
    "print(fdist_clean.most_common(10))\n",
    "fdist_clean.plot(10)"
   ]
  },
  {
   "source": [
    "# Wordcloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate(text)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "source": [
    "# Stemming"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porter's stemmer\n",
    "porter = PorterStemmer()\n",
    "words_study = [\"Study\", \"Studying\", \"Studies\", \"Studied\"]\n",
    "study = [porter.stem(w) for w in words_study]\n",
    "print(study)\n",
    "\n",
    "words_random = [\"Studies\", \"leaves\", \"decreases\", \"plays\"]\n",
    "random = [porter.stem(w) for w in words_random]\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowballstemmer languages\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "source": [
    "# Lemmatizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "word_be = [\"am\", \"is\", \"are\", \"was\", \"were\"]\n",
    "\n",
    "be = [lemma.lemmatize(w, pos=\"v\") for w in word_be] # pos = part of speech\n",
    "print(be)"
   ]
  },
  {
   "source": [
    "# Part of Speech"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"A very beautiful young lady is walking on the beach\"\n",
    "\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "\n",
    "nltk.pos_tag(tokenized_words)"
   ]
  },
  {
   "source": [
    "# Named Entity Recognition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (PERSON Mr./NNP)\n  (PERSON Smith/NNP)\n  made/VBD\n  a/DT\n  deal/NN\n  on/IN\n  a/DT\n  beach/NN\n  of/IN\n  (GPE Switzerland/NNP)\n  near/IN\n  (ORGANIZATION WHO/NNP))\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Mr. Smith made a deal on a beach of Switzerland near WHO\"\n",
    "\n",
    "tokenized_words = word_tokenize(sentence)\n",
    "tagged_words = nltk.pos_tag(tokenized_words)\n",
    "n_e_r = nltk.ne_chunk(tagged_words, binary=False)\n",
    "print(n_e_r)"
   ]
  },
  {
   "source": [
    "# Bag of Words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Jim and Pam travelled by the bus\",\n",
    "    \"The train was late\",\n",
    "    \"The flight was full. Travelling by flight is expensive\"\n",
    "]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sentences).toarray()\n",
    "\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}