{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0e3cc630a27171bda4963781135d38e3c4b2db62d9414523c30df1024dbff97fb",
   "display_name": "Python 3.8.8  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e3cc630a27171bda4963781135d38e3c4b2db62d9414523c30df1024dbff97fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "source": [
    "**Improvements**\n",
    "\n",
    "- Name Entity Recognition to remove those\n",
    "- PoS, only look at e.g. adjectives?\n",
    "- Other ML-modell than logreg?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Crunch\n",
    "Only run when adding new stuff to cleaner, otherwise use the CSV in the next section"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading original\n",
    "original = pd.read_csv('labeledTrainData.tsv', sep=\"\\t\")\n",
    "original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    words = word_tokenize(text)\n",
    "    words_no_punc = [word.lower() for word in words if word.isalpha()]\n",
    "    no_stop = [word for word in words_no_punc if word not in stops]\n",
    "    stems = [porter.stem(word) for word in no_stop]\n",
    "    clean = ' '.join(stems)\n",
    "\n",
    "    return clean\n",
    "\n",
    "original[\"review_clean\"] = original[\"review\"].apply(lambda text: clean_text(text))\n",
    "original.to_csv('train_clean.csv', index=False)"
   ]
  },
  {
   "source": [
    "# Exploration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_clean.csv', usecols=[\"sentiment\", \"review\", \"review_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment: 0\n\nBefore (1214 chars):\nSeriously, the fact that this show is so popular just boggles the mind. This show isn't funny, it isn't clever, it isn't original, it's just a steaming pile of bull crap. Let me start with the charact\n\nAfter (627 chars):\nserious fact show popular boggl mind show funni clever origin steam pile bull crap let start charact charact moron loud exagger voic sound like fingernail blackboard voic act could better anim god hur\n"
     ]
    }
   ],
   "source": [
    "random_nr = random.randint(0, len(df))\n",
    "\n",
    "old = df[\"review\"][random_nr]\n",
    "new = df[\"review_clean\"][random_nr]\n",
    "\n",
    "print(f'Sentiment: {df[\"sentiment\"][random_nr]}')\n",
    "print(f'\\nBefore ({len(old)} chars):')\n",
    "print(old[:200])\n",
    "print(f'\\nAfter ({len(new)} chars):')\n",
    "print(new[:200])"
   ]
  },
  {
   "source": [
    "# Train and test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review_clean'], df['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Logistic regression\n",
    "~87% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.1s finished\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, verbose=2)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Fitting\n",
    "logreg = LogisticRegression(max_iter=1000, verbose=2)\n",
    "\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2111,  301],\n",
       "       [ 381, 2207]])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "pred_logreg = logreg.predict(X_test)\n",
    "confusion_matrix(pred_logreg, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.85      0.88      0.86      2412\n           1       0.88      0.85      0.87      2588\n\n    accuracy                           0.86      5000\n   macro avg       0.86      0.86      0.86      5000\nweighted avg       0.86      0.86      0.86      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_logreg, y_test))"
   ]
  },
  {
   "source": [
    "# KNN\n",
    "~63% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting optimal nr of neighbors (takes ~2 mins)\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator=KNeighborsClassifier(), \n",
    "    param_grid={'n_neighbors': np.arange(3,11)}, # surely not less than 3, 11 as max due to time consumption\n",
    "    verbose=2,\n",
    "    cv=3\n",
    ")\n",
    "knn_grid.fit(X_train, y_train)\n",
    "optimal_neighbors = knn_grid.best_params_['n_neighbors']\n",
    "optimal_neighbors # Will return 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=9)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=9) # optimal_neighbors w/o having to run it\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1503,  817],\n",
       "       [ 989, 1691]])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "pred_knn = knn.predict(X_test)\n",
    "confusion_matrix(pred_knn, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.60      0.65      0.62      2320\n           1       0.67      0.63      0.65      2680\n\n    accuracy                           0.64      5000\n   macro avg       0.64      0.64      0.64      5000\nweighted avg       0.64      0.64      0.64      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_knn, y_test))"
   ]
  },
  {
   "source": [
    "# Naive Bayes\n",
    "~65% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2004, 1265],\n",
       "       [ 488, 1243]])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pred_nb = nb.predict(X_test.toarray())\n",
    "confusion_matrix(pred_nb, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.80      0.61      0.70      3269\n           1       0.50      0.72      0.59      1731\n\n    accuracy                           0.65      5000\n   macro avg       0.65      0.67      0.64      5000\nweighted avg       0.70      0.65      0.66      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_nb, y_test))"
   ]
  },
  {
   "source": [
    "# Random Forest\n",
    "~85% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=50)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "forest = RandomForestClassifier(max_depth=50)\n",
    "\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2070,  343],\n",
       "       [ 422, 2165]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "pred_forest = forest.predict(X_test)\n",
    "confusion_matrix(pred_forest, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.83      0.86      0.84      2413\n           1       0.86      0.84      0.85      2587\n\n    accuracy                           0.85      5000\n   macro avg       0.85      0.85      0.85      5000\nweighted avg       0.85      0.85      0.85      5000\n\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(pred_forest, y_test))"
   ]
  }
 ]
}