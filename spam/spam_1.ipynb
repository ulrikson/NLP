{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0e3cc630a27171bda4963781135d38e3c4b2db62d9414523c30df1024dbff97fb",
   "display_name": "Python 3.8.8  ('venv': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e3cc630a27171bda4963781135d38e3c4b2db62d9414523c30df1024dbff97fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import scipy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "source": [
    "# Importing and cleaning data\n",
    "Only run when adding new stuff to cleaner, otherwise use spam_clean.csv"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('spam.csv', usecols=[0,1])\n",
    "df_1.columns=['spam', 'text']\n",
    "\n",
    "df_1['spam'] = df_1['spam'].apply(lambda x: 1 if x=='spam' else 0) # spam = 1, ham = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"english\")\n",
    "porter = PorterStemmer()\n",
    "\n",
    "nr_reviews = df_1['text'].size\n",
    "\n",
    "def clean_text(text, index):\n",
    "    text = bs4(text).get_text()\n",
    "    words = word_tokenize(text)\n",
    "    words_no_punc = [word.lower() for word in words if word.isalpha()]\n",
    "    no_stop = [word for word in words_no_punc if word not in stops]\n",
    "    stems = [porter.stem(word) for word in no_stop]\n",
    "    clean = ' '.join(stems)\n",
    "\n",
    "    index = index + 1\n",
    "    if ((index)%500 == 0):\n",
    "        print('\\r', end='')\n",
    "        progress = int(100*index/nr_reviews)\n",
    "        print(f'Processing: {progress}%', end=' ')\n",
    "\n",
    "    return clean\n",
    "\n",
    "df_1[\"clean\"] = df_1.apply(lambda row: clean_text(row['text'], row.name), axis=1)\n",
    "print('\\rProcessing: 100%')\n",
    "df_1.to_csv('spam_clean.csv', index=False)\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "# Train, test and vectorizing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spam_clean.csv', usecols=[\"spam\", \"clean\"])\n",
    "df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True) # removing NaNs\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean'], df['spam'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words\n",
    "bow = CountVectorizer()\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "source": [
    "# Logistic regression\n",
    "BoW: 98%, TFIDF: 96% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       985\n",
      "           1       0.91      0.99      0.95       127\n",
      "\n",
      "    accuracy                           0.99      1112\n",
      "   macro avg       0.96      0.99      0.97      1112\n",
      "weighted avg       0.99      0.99      0.99      1112\n",
      "\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# BoW\n",
    "logreg_bow = LogisticRegression(max_iter=1000, verbose=2)\n",
    "logreg_bow.fit(X_train_bow, y_train)\n",
    "pred_logreg_bow = logreg_bow.predict(X_test_bow)\n",
    "\n",
    "print(classification_report(pred_logreg_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      1005\n",
      "           1       0.75      0.97      0.85       107\n",
      "\n",
      "    accuracy                           0.97      1112\n",
      "   macro avg       0.88      0.97      0.92      1112\n",
      "weighted avg       0.97      0.97      0.97      1112\n",
      "\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "# TFIDF\n",
    "logreg_tfidf = LogisticRegression(max_iter=1000, verbose=2)\n",
    "logreg_tfidf.fit(X_train_tfidf, y_train)\n",
    "pred_logreg_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "print(classification_report(pred_logreg_tfidf, y_test))"
   ]
  },
  {
   "source": [
    "# ANN\n",
    "BoW: 99%, TFIDF: 99% accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "139/139 - 1s - loss: 0.4939 - accuracy: 0.8932\n",
      "Epoch 2/5\n",
      "139/139 - 0s - loss: 0.1379 - accuracy: 0.9802\n",
      "Epoch 3/5\n",
      "139/139 - 0s - loss: 0.0559 - accuracy: 0.9892\n",
      "Epoch 4/5\n",
      "139/139 - 0s - loss: 0.0322 - accuracy: 0.9939\n",
      "Epoch 5/5\n",
      "139/139 - 0s - loss: 0.0214 - accuracy: 0.9955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.99      1.00       982\n",
      "        True       0.94      1.00      0.97       130\n",
      "\n",
      "    accuracy                           0.99      1112\n",
      "   macro avg       0.97      1.00      0.98      1112\n",
      "weighted avg       0.99      0.99      0.99      1112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ann_bow = Sequential()\n",
    "ann_bow.add(Dense(10, activation = 'relu'))\n",
    "ann_bow.add(Dense(10, activation = 'relu'))\n",
    "ann_bow.add(Dense(1, activation = 'sigmoid'))\n",
    "ann_bow.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
    "ann_bow.fit(X_train_bow.toarray(), y_train, batch_size=32, epochs=5, verbose=2)\n",
    "\n",
    "pred_ann_bow = ann_bow.predict(X_test_bow) > 0.5\n",
    "print(classification_report(pred_ann_bow, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "139/139 - 1s - loss: 0.5152 - accuracy: 0.8495\n",
      "Epoch 2/5\n",
      "139/139 - 0s - loss: 0.1920 - accuracy: 0.9377\n",
      "Epoch 3/5\n",
      "139/139 - 0s - loss: 0.0799 - accuracy: 0.9811\n",
      "Epoch 4/5\n",
      "139/139 - 0s - loss: 0.0454 - accuracy: 0.9890\n",
      "Epoch 5/5\n",
      "139/139 - 0s - loss: 0.0295 - accuracy: 0.9928\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       978\n",
      "        True       0.93      0.96      0.95       134\n",
      "\n",
      "    accuracy                           0.99      1112\n",
      "   macro avg       0.96      0.98      0.97      1112\n",
      "weighted avg       0.99      0.99      0.99      1112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ann_tfidf = Sequential()\n",
    "ann_tfidf.add(Dense(10, activation = 'relu'))\n",
    "ann_tfidf.add(Dense(10, activation = 'relu'))\n",
    "ann_tfidf.add(Dense(1, activation = 'sigmoid'))\n",
    "ann_tfidf.compile(optimizer = 'Adam', loss ='binary_crossentropy', metrics = ['accuracy'])\n",
    "ann_tfidf.fit(X_train_tfidf.toarray(), y_train, batch_size=32, epochs=5, verbose=2)\n",
    "\n",
    "pred_ann_tfidf = ann_tfidf.predict(X_test_tfidf.toarray()) > 0.5\n",
    "print(classification_report(pred_ann_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}